{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739870aa-e377-4a22-b4f1-616fd97074a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove input cells at runtime (nbsphinx)\n",
    "import IPython.core.display as d\n",
    "d.display_html('<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>', raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe906d6c-e373-408f-a566-c43ae9b47300",
   "metadata": {},
   "source": [
    "# Model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25108101-419c-469f-9ca0-95a83c8f099b",
   "metadata": {},
   "source": [
    "**Recommended datasample(s):**\n",
    "Depending on the model to validate,\n",
    "- ``gamma-1`` (dataset used to build the energy model)\n",
    "- ``gamma-2`` (dataset used to build the energy classifier)\n",
    "\n",
    "**Data level:** DL1b (telescope-wise image parameters) + DL2a (only shower geometry for energy estimation, plus estimated energy for particle classification)\n",
    "\n",
    "**Description:**\n",
    "\n",
    "This notebook contains benchmarks which are aimed at evaluating the performance of the models\n",
    "_before_ committing to them.\n",
    "\n",
    "As such, this notebook actually trains, tests and validates _more than one_ instance of the model of choice (defined by its configuration file in the current analysis directory).\n",
    "This is the main reason why the notebook has been separated from the main ones, which should validate only the final realization of the models (i.e. the one that will be actually used to apply the estimates in the subsequent steps of the analysis).\n",
    "\n",
    "**Requirements and steps to reproduce:**\n",
    "\n",
    "To run this notebook you will need a TRAINING file generated using ``protopipe-TRAINING``.\n",
    "\n",
    "To get a filled notebook and reproduce these results,\n",
    "\n",
    "- get the necessary input files using ``protopipe-TRAINING`` (see documentation)\n",
    "- execute the notebook with ``protopipe-BENCHMARK``,\n",
    "\n",
    "``protopipe-BENCHMARK launch --config_file configs/benchmarks.yaml -n MODELS/benchmarks_MODELS_tuning``\n",
    "\n",
    "To obtain the list of all available parameters add ``--help-notebook``.\n",
    "\n",
    "**Development and testing:**  \n",
    "\n",
    "As with any other part of _protopipe_ and being part of the official repository, this notebook can be further developed by any interested contributor.   \n",
    "The execution of this notebook is not currently automatic, it must be done locally by the user _before_ pushing a pull-request.  \n",
    "Please, strip the output before pushing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae3472c-fda1-4ee9-bdf0-842e6c53910c",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc8c553-b6e2-45bc-8b2d-149384e50c67",
   "metadata": {},
   "source": [
    "* [Validation curves](#Validation-curves)\n",
    "* [Learning curves](#Learning-curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ae260-706a-4efd-a4ef-6948755b0d37",
   "metadata": {},
   "source": [
    "## Imports\n",
    "[back to top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf20bfc8-07ce-4be3-a200-8362635a7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gzip\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from protopipe.pipeline.utils import load_config\n",
    "from protopipe.mva import TrainModel\n",
    "from protopipe.mva.utils import make_cut_list, prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728aa1e1-e41f-4622-b2b5-d14d2de03e8e",
   "metadata": {},
   "source": [
    "## Model generation\n",
    "[back to top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b6404-6b3b-4e05-9a6b-b13ab03098d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses_directory = \"\"\n",
    "analysis_name = \"\"\n",
    "camera = \"LSTCam\" # Camera model to validate\n",
    "estimated_quantity = \"energy\" # energy or classification\n",
    "model_configuration_filename = \"RandomForestRegressor.yaml\" # RandomForestRegressor.yaml, AdaBoostRegressor.yaml or RandomForestClassifier.yaml\n",
    "output_directory = Path.cwd() # default output directory for plots\n",
    "max_events = 10000 # Integer number of maximum events to consider from input data (None == max, default: 10000)\n",
    "param_name = \"max_depth\" # Name of the parameter against which evaluate the model\n",
    "param_range = [10,20,30,40,50] # Name of the parameter against which evaluate the model\n",
    "scoring = \"explained_variance\" # e.g. \"explained_variance\", \"roc_auc\", ... (see https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)\n",
    "cv = 2 # Cross-Validation (CV)method: int for int, list [n_splits, train_size] for ShuffleSplit (default: 2-fold cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7991cf2-66d2-45b6-8244-f75200f2dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we check if a _plots_ folder exists already.  \n",
    "# If not, we create it.\n",
    "plots_folder = Path(output_directory) / \"plots\"\n",
    "plots_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Plot aesthetics settings\n",
    "\n",
    "style.use(matplotlib_settings[\"style\"])\n",
    "cmap = matplotlib_settings[\"cmap\"]\n",
    "\n",
    "if matplotlib_settings[\"style\"] == \"seaborn-colorblind\":\n",
    "    \n",
    "    colors_order = ['#0072B2', '#D55E00', '#F0E442', '#009E73', '#CC79A7', '#56B4E9']\n",
    "    rc('axes', prop_cycle=cycler(color=colors_order))\n",
    "\n",
    "if use_seaborn:\n",
    "    import seaborn as sns\n",
    "\n",
    "    sns.set_theme(context=seaborn_settings[\"theme\"][\"context\"] if \"context\" in seaborn_settings[\"theme\"] else \"talk\",\n",
    "                  style=seaborn_settings[\"theme\"][\"style\"] if \"style\" in seaborn_settings[\"theme\"] else \"whitegrid\",\n",
    "                  palette=seaborn_settings[\"theme\"][\"palette\"] if \"palette\" in seaborn_settings[\"theme\"] else None,\n",
    "                  font=seaborn_settings[\"theme\"][\"font\"] if \"font\" in seaborn_settings[\"theme\"] else \"Fira Sans\",\n",
    "                  font_scale=seaborn_settings[\"theme\"][\"font_scale\"] if \"font_scale\" in seaborn_settings[\"theme\"] else 1.0,\n",
    "                  color_codes=seaborn_settings[\"theme\"][\"color_codes\"] if \"color_codes\" in seaborn_settings[\"theme\"] else True\n",
    "                  )\n",
    "    \n",
    "    sns.set_style(seaborn_settings[\"theme\"][\"style\"], rc=seaborn_settings[\"rc_style\"])\n",
    "    sns.set_context(seaborn_settings[\"theme\"][\"context\"],\n",
    "                    font_scale=seaborn_settings[\"theme\"][\"font_scale\"] if \"font_scale\" in seaborn_settings[\"theme\"] else 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca5958-b478-4d4d-9187-79ac5f4113b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CV\n",
    "if ((type(cv) is int) or (cv is None)) and (estimated_quantity==\"classification\"):\n",
    "    if cv is None:\n",
    "        cv = 5\n",
    "    print(f\"Selected CV method = {cv}-StratifiedKFold cross validation\")\n",
    "    cv_to_use = cv\n",
    "elif ((type(cv) is int) or (cv is None)) and (estimated_quantity!=\"classification\"):\n",
    "    if cv is None:\n",
    "        cv = 5\n",
    "    print(f\"Selected CV method = {cv}-KFold cross validation\")\n",
    "    cv_to_use = cv\n",
    "elif type(cv) is list:\n",
    "    print(f\"Selected CV method = ShuffleSplit with {cv[0]} splits and a train size of {cv[1]}\")\n",
    "    cv_to_use = ShuffleSplit(n_splits=cv[0], train_size=cv[1], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2c74f-6d8c-4b67-983f-d1490f92e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configuration_path = Path(analyses_directory) / analysis_name / \"configs\" / model_configuration_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0980db-7352-47e3-a501-2dc80954f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration files\n",
    "cfg = load_config(model_configuration_path)\n",
    "\n",
    "# Get info from configs\n",
    "# Map model types to the models supported by the script\n",
    "model_types = {\"regressor\": [\"RandomForestRegressor.yaml\",\n",
    "                                 \"AdaBoostRegressor.yaml\"],\n",
    "                \"classifier\": [\"RandomForestClassifier.yaml\"]}\n",
    "\n",
    "def get_key(my_dict, val):\n",
    "    for key, value in my_dict.items():\n",
    "        if val in value:\n",
    "             return key\n",
    "\n",
    "model_type = get_key(model_types, model_configuration_filename)\n",
    "method_name = cfg[\"Method\"][\"name\"].split(\".\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb06b8-8e1c-4f96-85ef-70e5030e6681",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = cfg[\"Method\"][\"target_name\"]\n",
    "\n",
    "if model_type == \"regressor\":\n",
    "    try:\n",
    "        log_10_target = cfg[\"Method\"][\"log_10_target\"]\n",
    "    except KeyError:\n",
    "        log_10_target = True\n",
    "\n",
    "    if log_10_target:\n",
    "        target_name = f\"log10_{target_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00616281-92b6-4cf1-a484-f24a3a75ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_basic = cfg[\"FeatureList\"][\"Basic\"]\n",
    "features_derived = cfg[\"FeatureList\"][\"Derived\"]\n",
    "features = features_basic + list(features_derived)\n",
    "features = sorted(features)\n",
    "print(f\"Model features = {features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff7bec0-085c-4e0c-8c5e-7d3cb12d1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is always at least one (main) model to initialize\n",
    "model_to_use = cfg['Method']['name']\n",
    "module_name = '.'.join(model_to_use.split('.', 2)[:-1])\n",
    "class_name = model_to_use.split('.')[-1]\n",
    "module = importlib.import_module(module_name)  # sklearn.XXX\n",
    "model = getattr(module, class_name)\n",
    "print(f\"Going to use {module_name}.{class_name}...\")\n",
    "\n",
    "# Check for any base estimator if main model is a meta-estimator\n",
    "if \"base_estimator\" in cfg['Method']:\n",
    "    base_estimator_cfg = cfg['Method']['base_estimator']\n",
    "    base_estimator_name = base_estimator_cfg['name']\n",
    "    base_estimator_pars = base_estimator_cfg['parameters']\n",
    "    base_estimator_module_name = '.'.join(base_estimator_name.split('.', 2)[:-1])\n",
    "    base_estimator_class_name = base_estimator_name.split('.')[-1]\n",
    "    base_estimator_module = importlib.import_module(base_estimator_module_name)  # sklearn.XXX\n",
    "    base_estimator_model = getattr(base_estimator_module, base_estimator_class_name)\n",
    "    initialized_base_estimator = base_estimator_model(**base_estimator_pars)\n",
    "    print(f\"...based on {base_estimator_module_name}.{base_estimator_class_name}\")\n",
    "    initialized_model = model(base_estimator=initialized_base_estimator,\n",
    "                              **cfg['Method']['tuned_parameters'])\n",
    "else:\n",
    "    initialized_model = model(**cfg['Method']['tuned_parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dccc98-0ce0-4709-be63-2b8b0c806e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinpoint the data\n",
    "input_data_path = Path(analyses_directory) / analysis_name / Path(\"data/TRAINING/for_energy_estimation\")\n",
    "signal_input_file = \"TRAINING_energy_tail_gamma_merged.h5\"\n",
    "# Get the selection cuts for signal\n",
    "sig_cuts = make_cut_list(cfg[\"SigFiducialCuts\"])\n",
    "\n",
    "signal_before_cuts = pd.read_hdf(input_data_path / signal_input_file, camera, mode=\"r\")\n",
    "print(f\"Number of signal images available before cuts: {len(signal_before_cuts)}\")\n",
    "\n",
    "data_sig = prepare_data(ds=signal_before_cuts,\n",
    "                        derived_features=features_derived,\n",
    "                        select_data=True,\n",
    "                        cuts=sig_cuts)\n",
    "    \n",
    "# Select data and add derived features\n",
    "if estimated_quantity == \"classification\":\n",
    "    \n",
    "    input_data_path = Path(analyses_directory) / analysis_name / Path(\"data/TRAINING/for_particle_classification\")\n",
    "    background_input_file = \"TRAINING_classification_tail_proton_merged.h5\" \n",
    "    \n",
    "    background_before_cuts = pd.read_hdf(input_data_path / background_input_file, camera, mode=\"r\")\n",
    "    print(f\"Number of background images available before cuts: {len(background_before_cuts)}\")\n",
    "    \n",
    "    # Get the selection cuts for background\n",
    "    bkg_cuts = make_cut_list(cfg[\"BkgFiducialCuts\"])\n",
    "    data_bkg = prepare_data(ds=background_before_cuts,\n",
    "                            label=0,\n",
    "                            derived_features=features_derived,\n",
    "                            select_data=True,\n",
    "                            cuts=bkg_cuts)\n",
    "    \n",
    "if max_events:\n",
    "    data_sig = data_sig[0:max_events]\n",
    "    if estimated_quantity == \"classification\":\n",
    "        data_bkg = data_bkg[0:max_events]\n",
    "\n",
    "print(f\"Number of signal images available after cuts: {len(data_sig)} ({len(data_sig)/len(signal_before_cuts)*100:.2f}%)\")\n",
    "if estimated_quantity == \"classification\":\n",
    "    print(f\"Number of background images available after cuts: {len(data_bkg)} ({len(data_bkg)/len(background_before_cuts)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe7cfc-d954-44ef-9f4c-b7306bb43e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training features values\n",
    "# Get the target values\n",
    "if model_type == \"classifier\":\n",
    "    X = pd.concat([data_sig[features], data_bkg[features]])\n",
    "    y = pd.concat([data_sig[target_name], data_bkg[target_name]])\n",
    "else:\n",
    "    X = data_sig[features]\n",
    "    y = data_sig[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8ec45-6336-4af7-b994-899e03500a09",
   "metadata": {},
   "source": [
    "## Validation curves\n",
    "[back to top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438fa6d-6454-4976-9145-70783b84401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_range = [10, 20, 30, 40, 50]\n",
    "print(f\"Range of {param_name} = {param_range}\")\n",
    "\n",
    "if (param_name is None) or (param_range is None):\n",
    "    raise ValueError(\"Validation parameter or its range is undefined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee39f97-78a9-40f8-b370-964b8a950fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, test_scores = validation_curve(initialized_model,\n",
    "                                             X,\n",
    "                                             y,\n",
    "                                             param_name=param_name,\n",
    "                                             param_range=param_range,\n",
    "                                             cv=cv,\n",
    "                                             scoring=scoring, \n",
    "                                             n_jobs=-1\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f0d92-6831-4ddc-8443-cfe8cb6e619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3162d102-f5a8-4e5c-a3e7-74a6f7830128",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(f\"Validation Curve with {module_name}.{class_name}\")\n",
    "plt.xlabel(f\"{param_name}\")\n",
    "plt.ylabel(f\"Score ({scoring})\")\n",
    "lw = 2\n",
    "plt.ylim(-0.2, 0)\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.savefig(f\"protopipe_test_model_validation_curve_{estimated_quantity}_{param_name}_{model_configuration_filename.split('.')[0]}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee102f71-89bc-4bb5-9bb0-fa2e4014b43b",
   "metadata": {},
   "source": [
    "## Learning curves\n",
    "[back to top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20bec7e-bb05-45a4-bce3-a5c2b08c8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_learning_curve(estimator, X, y, cv=None,\n",
    "                        scoring = None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Compute mean and standard deviation of train scores, test scores and fit times,\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       scoring = scoring,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    return (train_sizes,\n",
    "            train_scores_mean, train_scores_std,\n",
    "            test_scores_mean, test_scores_std,\n",
    "            fit_times_mean, fit_times_std)\n",
    "\n",
    "def plot_learning_curve(train_sizes, train_scores_mean, train_scores_std,\n",
    "                        test_scores_mean, test_scores_std,\n",
    "                        fit_times_mean, fit_times_std,\n",
    "                        scoring, title=None, axes=None, ylim=None\n",
    "                        ):\n",
    "    \"\"\"Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    scoring: str or callable\n",
    "        Scoring function\n",
    "    title : str, default: None\n",
    "        Title for the chart.\n",
    "    axes : array-like of shape (3,), default: None\n",
    "        Axes to use for plotting the curves.\n",
    "    ylim : tuple of shape (2,), default: None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "    train_scores_mean: np.array\n",
    "        Mean of the train scores\n",
    "    train_scores_std: np.array\n",
    "        Standard deviation of the train scores\n",
    "    test_scores_mean: np.array\n",
    "        Mean of the test scores\n",
    "    test_scores_std: np.array\n",
    "        Standard deviation of the test scores\n",
    "    fit_times_mean: np.array\n",
    "        Mean of the times spent for fitting in seconds\n",
    "    fit_times_std: np.array\n",
    "        Standard deviation of the times spent for fitting in seconds\n",
    "    \"\"\"\n",
    "    \n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(f\"Score ({scoring})\")\n",
    "    \n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"Times spent for fitting (seconds)\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"Times spent for fitting (seconds)\")\n",
    "    axes[2].set_ylabel(f\"Score ({scoring})\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "    #if ylim is not None:\n",
    "    #    axes[2].set_ylim(*ylim)\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d88990-e0c8-4604-b481-f4a687e629e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_sizes,\n",
    " train_scores_mean,\n",
    " train_scores_std,\n",
    " test_scores_mean,\n",
    " test_scores_std,\n",
    " fit_times_mean,\n",
    " fit_times_std) = compute_learning_curve(model(**cfg['Method']['tuned_parameters']), X, y, cv=cv, scoring=scoring, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883454b8-be47-4c52-b6e2-dc78651ffa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(8, 16))\n",
    "plt.subplots_adjust(hspace=0.35)\n",
    "\n",
    "title = f\"Learning Curves for {estimated_quantity} with {model_configuration_filename.split('.')[0]}\"\n",
    "\n",
    "plot_learning_curve(train_sizes,\n",
    "                    train_scores_mean, train_scores_std,\n",
    "                    test_scores_mean, test_scores_std,\n",
    "                    fit_times_mean, fit_times_std,\n",
    "                    scoring,\n",
    "                    title,\n",
    "                    axes,\n",
    "                    ylim=(-0.05,0)\n",
    "                    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64c479-a2db-44a1-b547-b27ade0e85f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
