.. _beforepushing:

Mandatory checks
================

This page illustrates all the information regarding what to check before and
while you push your modifications.

The development of this software is monitored by a Continuous Integration (CI)
pipeline that takes care of all checks (some using external services).

.. contents:: Index
    :local:
    :depth: 2

Documentation
-------------

Each Pull Request (PR) has to have its own documentation updates (if any),
according to what are the changes to be merged into master.

The documentation is generated by `Sphinx <https://www.sphinx-doc.org/en/master/>`__
with `reStructuredText <https://docutils.sourceforge.io/rst.html>`__ syntax.

To build and check your documentation locally,

- cd ``docs``
- for big changes (or just to be sure), ``rm api/* && make clean && make html``
- for small changes, ``make html``

The built documentation will be stored under ``docs/_build/html`` from which you
can open ``index.html`` with your favorite browser.

You will have to fix any warning that appears during documentation building,
because the documentation also runs on `readthedocs <https://readthedocs.org/>`__
with an option to treat warnings as errors.

Testing
-------

All testing code is called by issuing the ``pytest`` command.

This command can be called from any place within the cloned repository and it
will always run from the root directory of the project.

For debugging purposes you can add the ``-s`` option which will allow to 
visualise any ``print`` statement within the test module(s).

Testing is automatically triggered by the CI every time a new
pull-request is pushed to the repository, and its correct
execution is one of the mandatory condition for merging.

Unit tests
^^^^^^^^^^

You can follow 
`these guidelines <https://cta-observatory.github.io/ctapipe/development/code-guidelines.html#unit-tests>`__
to understand what a unit-test is supposed to do.

.. note::
  This is a maintenance activity which has being long overdue and we need
  manpower for it, so if you have experience on this or you want to contribute
  please feel free to do so.

  For more information on how to contribute to this effort check
  `this issue <https://github.com/cta-observatory/protopipe/issues/69>`__.

Being *protopipe* based on *ctapipe*, all the tools imported from the latter
have been already tested and approved (*protopipe* uses always a version of
*ctapipe* which has been released on the Anaconda framework).
Same for *pyirf*.

.. warning::
  This is not true for,

  - hard-coded parts that had to be modified in anticipation of code migration,
  - *protopipe* functions themselves (which will eventually migrate to *ctapipe*)

  Regarding the first point: given the difference in versions between the
  imported *ctapipe* and its development version, sometimes it's possible that, in
  order to code a new feature, this has to be pull-requested to *ctapipe* and at
  the same time hardcoded in *protopipe*, until the new version of *ctapipe* is released.

Integration tests
^^^^^^^^^^^^^^^^^

These are neither unit-tests nor benchmarks, but rather functions that test
whole functionalities and not just single API functions.

In the case of the pipeline, such functionalities are the scripts/tools
that make up its workflow.

.. note::
  For more information on how to contribute to this effort check
  `this issue <https://github.com/cta-observatory/protopipe/issues/70>`__.

The integration tests are defined in the dedicated module ``pipeline/scripts/tests/test_pipeline.py``
and start from test simtel files stored on a CC-IN2P3 dataserver.  

The test data is diffuse data from the Prod3b baseline simulations of both 
CTAN and CTAS produced with the following Corsika settings,

- gammas, ``NSHOW=10 ESLOPE=-2.0 EMIN=10 EMAX=20 NSCAT=1 CSCAT=200 VIEWCONE=3``
- protons, ``NSHOW=10 ESLOPE=-2.0 EMIN=100 EMAX=200 NSCAT=1 CSCAT=200 VIEWCONE=3``
- electrons, ``NSHOW=10 ESLOPE=-2.0 EMIN=10 EMAX=20 NSCAT=1 CSCAT=200 VIEWCONE=3``

and it is analysed using the same workflow as in a standard full-scale analysis.

Benchmarks
----------

Benchmarks are a way to visualize qualitatively and quantitatively the performace
of each step of an analysis.

*protopipe* provides a set of benchmarking notebooks within the ``benchmarks`` module.
Such notebooks make use of the material stored under the same module (see :ref:`benchmarks`).

.. warning::

  The refactoring and update with old material is not finished.
  It is possible that some notebooks make use of new features as well as present
  few bugs.

  A notebook template will be added soon.

Their contents followed initially the development triggered by the 
comparison between *protopipe* and the historical pipelines *CTA-MARS* (see
`this issue <https://github.com/cta-observatory/protopipe/issues/24>`__ and
references therein for a summary), and *EventDisplay*.
They have been continuously improved and they are expected to evolve in time,
especially with the progressing refactoring with *ctapipe*.

For the moment the purpose of these tools is to help users and
developers to check if their changes improve or degrade
previous performances.

Any developer interested in contributing to benchmarking 
can do so from a development installation of *protopipe*.
It is suggested to open the notebooks with ``jupyter lab``
from their location at ``protopipe/benchmarks/notebooks``.

Currently available benchmarks are organised as follows,

- TRAINING

  * Calibration
  * Image cleaning
  * Image intensity resolution
  * Direction Look-Up Tables
  * Direction reconstruction
  * to energy estimation
  * Energy Look-Up Tables
  * to classification

In particular:

  * calibration requires *ctapipe* DL1a output (images without parameters),
  * all image cleaning and direction reconstruction benchmarks use *protopipe*
    TRAINING data **without** estimated energy,
  * all benchmarks for the energy estimator use *protopipe* TRAINING data **without** estimated energy,
  * benchmarks for the classifier use *protopipe* TRAINING data **with** energy as the only estimated DL2b parameter.

- MODELS

These performances are obtained from a *test* portion of the TRAINING data,

  * Energy
  * Classification
  * Tuning

- DL2

  * Particle classification
  * Direction reconstruction

- DL3

  * Instrument Response Functions and sensitivity
  * Performance poster

The DL3 folder contains also the CTA requirements, while the ASWG performance
data is left to the user, being internal.

.. note::
  
  Remember that in the framework of CTA software there are similar projects,

  - `ctaplot <https://github.com/cta-observatory/ctaplot>`__ and
  - `cta-benchmarks <https://github.com/cta-observatory/cta-benchmarks>`__.

  Plots could be properly migrated-to/synchronized-with *ctaplot*, same for
  the single pipeline steps with *cta-benchmarks* after the pipeline has been refactored using
  *ctapipe*'s stage tools.
